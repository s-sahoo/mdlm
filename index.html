<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Simple and Effective Masked Diffusion Language Models">
  <meta property="og:title" content="MDLM Blog post"/>
  <meta property="og:description" content="Simple and Effective Masked Diffusion Language Models"/>
  <meta property="og:url" content="https://s-sahoo.com/mdlm"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/mdlm.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/mdlm.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Simple and Effective Masked Diffusion Language Models MDLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Simple and Effective Masked Diffusion Language Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/ganeshafavicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Simple and Effective <br> Masked Diffusion Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://s-sahoo.github.io" target="_blank">Subham Sekhar Sahoo</a>,</span>
                <span class="author-block">
                  <a href="https://mariannearriola.github.io" target="_blank">Marianne Arriola</a>,</span>
                  <span class="author-block">
                  <a href="https://yair-schiff.github.io" target="_blank">Yair Schiff</a>,</span>
                    <span class="author-block">
                      <a href="https://skylion007.github.io" target="_blank">Aaron Gokaslan</a>,</span>
                      <span class="author-block">
                        <a href="https://emarro.github.io" target="_blank">Edgar Marroquin</a>,</span>
                        <span class="author-block">
                          <a href="https://justinchiu.netlify.app" target="_blank">Justin T Chiu</a>,</span>
                            <span class="author-block">
                              <a href="https://rush-nlp.com" target="_blank">Alexander Rush</a>,</span>
                              <span class="author-block">
                                <a href="https://www.cs.cornell.edu/~kuleshov/" target="_blank">Volodymyr Kuleshov</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Cornell Tech, NY.
                      <!-- <br>Conferance name and year -->
                    </span>
                    
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                        <!-- ArXiv abstract Link -->
                        <span class="link-block">
                          <a href="https://arxiv.org/abs/2406.07524" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="ai ai-arxiv"></i>
                            </span>
                            <span>arXiv</span>
                          </a>
                        </span>

                        <!-- Github link -->
                        <span class="link-block">
                          <a href="https://github.com/kuleshov-group/mdlm" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fab fa-github"></i>
                          </span>
                          <span>Code</span>
                          </a>
                        </span>

                        <!-- Google Colab Link -->
                        <span class="link-block">
                          <a href="https://colab.research.google.com/drive/18nC6q7dWq154fI1BXPLwmtnS7Zvbrv6p?usp=sharing%2F" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                              <img src="https://colab.research.google.com/img/colab_favicon.ico" alt="Colab Logo" style="width: 18px; height: 18px;">
                            </span>
                            <span>Colab</span>
                          </a>
                        </span>

                        <!-- HuggingFace link -->
                        <span class="link-block">
                          <a href="https://huggingface.co/kuleshov-group/mdlm-owt" target="_blank" class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                              <p>&#129303;</p>
                            </span>
                            <span>HuggingFace</span>
                          </a>
                        </span>
                        </a>
                      </span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/mdlm.png" alt="MY ALT TEXT"/>
      <h4 class="subtitle has-text-centered">
        (<em>Left</em>) Our proposed masked diffusion language model (MDLM)
        is trained using a weighted average of masked cross entropy losses.
        (<em>Top Right</em>) In comparison to masked language models (MLM),
        MDLM's objective correspond to a principled variational lower bound,
        and supports generation via ancestral sampling.(<em>Bottom Right</em>)
        Perplexity (PPL) on One Billion Words benchmark.
      </h4>
      <img src="static/images/output.gif" width="100%" alt="A descriptive text for the GIF" >
      <h4 class="subtitle has-text-centered">
        The sample generation process begins with a sequence of all masked tokens. MDLM then replaces these masked tokens with actual tokens in a random order.
      </h4>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While diffusion models excel at generating high-quality images, prior work reports
            a significant performance gap between diffusion and autoregressive (AR) methods
            on language modeling. In this work, we show that simple masked discrete diffusion
            is more performant than previously thought. We apply an effective training recipe
            that improves the performance of masked diffusion models and derive a simplified,
            Rao-Blackwellized objective that results in additional improvements. Our objective
            has a simple form—it is a mixture of classical masked language modeling losses—
            and can be used to train encoder-only language models that admit efficient samplers,
            including ones that can generate arbitrary lengths of text semi-autoregressively
            like a traditional language model. On language modeling benchmarks, a range of
            masked diffusion models trained with modern engineering practices achieves a new
            state-of-the-art among diffusion models, and approaches AR perplexity.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Diffusion Models -->
<section class="section" id="DiffusionModels">
  <div class="container is-max-desktop">
  <div class="content is-medium">
    <h2 class="title">Diffusion Models</h2>
    <p>Diffusion models excel at producing realistic, high-quality images and have received significant attention as potential tools for generating discrete data such as text, biological sequences, and graphs. Unlike autoregressive (AR) approaches, diffusion-based methods are not constrained to generate data sequentially, and therefore have the potential to improve long-term planning, controllable generation, and sampling speed. However, discrete diffusion methods exhibit a performance gap relative to AR models, especially in language modeling. The standard measure of language modeling performance is log-likelihood: when controlling for parameter count, prior work reports a sizable log-likelihood gap between AR and diffusion models.
    </p>
  <!-- Continuous Diffusion -->  
    <!-- <h4 class="subtitle">Continuous Diffusion</h3>
    <p>
      Diffusion models are trained to iteratively undo a forward corruption process  q  that corrupts the clean data  $\mathbf{x} \in \mathbb{R}^{n}$  by adding Gaussian noise. In the reverse generation process, the trained model iteratively denoises the Gaussian noise to generate clean inputs that correspond to the input data distribution.
    </p>
     -->
    <!-- Discrete Diffusion -->
      <h4 class="subtitle">Discrete Diffusion</h3>
      <p>
        Applications of diffusion modeling to discrete data can be categorized into two broad areas. The first involves embedding discrete structures in continuous space and then performing the Gaussian diffusion defined above on these continuous representations. More related to our method are works that define a diffusion process directly on discrete structures. <a href="https://arxiv.org/abs/2107.03006">D3PM </a> introduces a framework with a Markov forward process \( q(z_t|z_{t−1}) = \text{Cat}(z_t; Q_t z_{t−1}) \), defined by the multiplication of matrices \( Q_t \in \mathbb{R}^{n \times n} \) over \( T \) discrete time steps. The matrix \( Q_t \) is designed such that \( Q_T \cdot Q_{T-1} \cdots Q_1 \mathbf{x} \) converges to a stationary distribution.
      </p>
  </div>
  </div>

</section>
<!-- End Diffusion Models -->

<!-- Simple Masked Diffusion Models -->
<section class="section" id="SimpleMaskedDiffusionModels">
  <div class="container is-max-desktop">
  <div class="content is-medium">
    <h2 class="title">Simple Masked Diffusion Models</h2>
    <p>While previous work on discrete diffusion supports general forward processes (e.g., general \(Q_t\) in
    D3PM), absorbing state (i.e., masking) diffusion consistently achieves the best performance.
    In this work, instead of supporting general noise processes, we focus on masking and derive tight
    Rao-Blackwellized objectives that outperform general approaches and do not require CTMC theory (for ex. <a href="https://arxiv.org/abs/2310.16834">SEDD</a>).
    We denote our
    overall approach as Masked Diffusion Language Models (MDLM).
    <!-- <h4>Sample Generation Process</h4>
    <p>The sample generation process begins with a sequence of all masked tokens. MDLM then replaces these masked tokens with actual tokens in a random order.
      <img src="static/images/output2.gif" alt="A descriptive text for the GIF">
    </p> -->
    </p>
    <h4 class="subtitle">Interpolating Masked Diffusion</h4>
    <p>We forcus on forward processes q that interpolate between clean data \(\mathbf{x} \in \mathcal{V}\),
      where \(\mathcal{V}\) is a set of all one-hot vectors, and a target
      distribution \(\text{Cat}(.; \boldsymbol{\mathit{\pi}})\).  This approach is a direct extention of Gaussian diffusion where the intermediate sample interpolates between the clean data and white noise. The \(q\) defines a sequence
      of increasingly noisy latent variables \(\mathbf{z}_t \in \mathcal{V}\), where the time step \(t\) runs from \(t = 0\) (least noisy) to \(t = 1\)
      (most noisy).
      
      The marginal of \(\mathbf{z}_t\) conditioned on \(\mathbf{x}\) at time \(t\) is given by 
      \(q(\mathbf{z}_t|x) = \text{Cat}(\mathbf{z}_t;\alpha_t \mathbf{x}+(1 - \alpha_t) \boldsymbol{\mathit{\pi}})\),
      where \(\alpha_t \in [0, 1]\) is a strictly decreasing function in t, with \(\alpha_{t=0} \approx 0\) and \(\alpha_{t=1} \approx 1\).
      
      In absorbing state diffusion, we set \(\boldsymbol{\mathit{\pi}} = \mathbf{m}\) where \(\mathbf{m} \in \mathcal{V}\) and \(\mathbf{m}_K=1\), with \(K^\text{th}\)category representing the special masked token, [MASK].
    </p>
    
    <h4>Reverse process</h4>
    <p>The specific parameterization for \(p_\theta(\mathbf{z}_s | \mathbf{z}_t)\) with \(0 \leq s < t \leq 1\) that we use is:
      \begin{align}\label{eqn:approx_posterior}
          p_\theta(\mathbf{z}_s | \mathbf{z}_t) =
          q(\mathbf{z}_s | \mathbf{z}_t, \mathbf{x} = \mathbf {x}_\theta (\mathbf{z}_t, t)) =
          \begin{cases}
            \text{Cat} (\mathbf{z}_s; \mathbf{z}_t), & \mathbf{z}_t \neq \mathbf{m}, \\
            \text{Cat} \left( \mathbf{z}_s; \frac{
              (1 - \alpha_s)\mathbf{m} + (\alpha_s - \alpha_t) \mathbf{x}_\theta (\mathbf{z}_t, t)}{1 - \alpha_t}\right). & \mathbf{z}_t= \mathbf{m},
          \end{cases}
      \end{align}
      Furthermore, we induce 2 key properties of the absorbing state diffusion process into our denoising
      model, \( \mathbf{x}_\theta (\mathbf{z}_t, t): \mathcal{V} \times [0, 1] \rightarrow \Delta^{K}\),  where \(\Delta^{K}\) denotes the \(K\)-simplex:
      <ol>
        <li>
          <b>Zero Masking Probabilities:</b> First, notice that by definition, \( \langle \mathbf{x}, \mathbf{m} \rangle = 0 \).
          For this reason, we design the denoising network such that 
          \( \langle \mathbf{x}_\theta (\mathbf{z}_t,t),\mathbf{m} \rangle = 0 \), i.e., we substitute the logit index corresponding to the [MASK] token with \(-\infty\).
        </li>
        <li>
          <b>Carry-Over Unmasking:</b> Second, if \( \mathbf{z}_t \) is unmasked, then we desire \( \mathbf{x}_\theta (\mathbf{z}_t, t) = \mathbf{z}_t \), i.e., unmasked latents are "carried over". We accomplish this by substituting the output of our network to simply copy unmasked inputs.
        </li>
      </ol>
      As discussed in the paper, each of these properties plays a crucial role simplifying the Diffusion objetive. We implement these as substitutions to the output of \(\mathbf{x}_\theta (\mathbf{z}_t, t)\), hence we call our parameterization SUBS.
    </p>
    <h4>Loss</h4>
    <p>
      <!-- Note that the Negative Evidence Lower Bound (NELBO) takes the following form for a diffusion model: \( \mathcal{L}_{\text{reconstruction}} + \mathcal{L}_{\text{diffusion}} + \mathcal{L}_{\text{prior}}\). As described in the paper, SUBS parameterization reduces the  \( \mathcal{L}_{\text{reconstruction}} \) to \(0\) and \(\alpha_{t = 1} = 0\) ensures that  \( \mathcal{L}_{\text{prior}} = 0 \). -->
      For a sequence \(\mathbf{x}^{1: L}\) of length \(L\), SUBS parameterization simplifies the Negative Evidence Lower Bound (NELBO) to the following:
      \begin{align}
          \mathcal{L}_{\text{NELBO}}^\infty = \mathbb{E}_{q}\int_{t=0}^{t=1} \frac{\alpha_{t}'}{1 - \alpha_t} \sum_{\ell = 1}^{L} \log \langle \mathbf {x}_\theta^\ell(\mathbf{z}_t), \mathbf{x}^\ell \rangle \text{d} t
      \end{align}
      where \(\alpha_{t}'\) denotes the time derivative of \(\alpha_{t}\). As shown in the table below, MDLM outperforms the previous diffusion models and nearly matches the performance of AR models in text generation on the LM1B dataset.

      <!-- <table>
        <caption>Test perplexities (PPL; &darr;) on LM1B. Best diffusion value is bolded. All perplexities for diffusion models are upper bounds.</caption>
        <thead>
          <tr>
            <th></th>
            <th>PPL (&darr;)</th>
            <th>OWT (&darr;)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>SEDD</td>
            <td>32.79</td>
            <td>24.10</td>
          </tr>
          <tr>
            <td>AR (Retrained)</td>
            <td>20.86</td>
            <td>17.54</td>
          </tr>
          <tr>
            <td><strong>MDLM (Ours)</strong></td>
            <td> <b>23.00</b></td>
            <td><b>23.21</b></td>
          </tr>
        </tbody>
      </table> -->
      <table>
        <caption><em>Test perplexities (PPL; &darr;) on LM1B. &dagger;. Best diffusion value is bolded.</em></caption>
        <thead>
          <tr>
            <th></th>
            <th></th>
            <th>Parameters</th>
            <th>PPL (&darr;)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="2"><i>Autoregressive</i></td>
            <td>Transformer-X Base</td>
            <td>0.46B</td>
            <td>23.5</td>
          </tr>
          <tr>
            <td><i>OmniNet<sub>T</sub></i></td>
            <td>100M</td>
            <td>21.5</td>
          </tr>
          <tr>
            <td rowspan="5"><i>Diffusion</i></td>
            <td>BERT-Mouth</td>
            <td>110M</td>
            <td>&le;142.89</td>
          </tr>
          <tr>
            <td>D3PM (absorb)</td>
            <td>70M</td>
            <td>&le;77.50</td>
          </tr>
          <tr>
            <td>Diffusion-LM</td>
            <td>80M</td>
            <td>&le;118.62</td>
          </tr>
          <tr>
            <td>DiffusionBert</td>
            <td>110M</td>
            <td>&le;63.78</td>
          </tr>
          <tr>
            <td>SEDD</td>
            <td>110M</td>
            <td>&le;32.79</td>
          </tr>
          <tr style="border-top: 0.1em solid;">
            <td rowspan="2"><i>Autoregressive (Retrained)</i></td>
            <td>Transformer (33B tokens)</td>
            <td rowspan="2">110M</td>
            <td>22.32</td>
          </tr>
          <tr>
            <td>Transformer (327B tokens)</td>
            <td>20.86</td>
          </tr>
          <tr>
            <td rowspan="2"><i>Diffusion (Ours)</i></td>
            <td><strong>MDLM </strong> (33B tokens)</td>
            <td rowspan="2">110M</td>
            <td>&le;27.04</td>
          </tr>
          <tr>
            <td><strong>MDLM </strong> (327B tokens)</td>
            <td>&le;<b>23.00</b></td>
          </tr>
        </tbody>
      </table>
      <br>
      <br>
      We also explore models' ability to generalize by taking models trained on OWT and evaluating how well they model unseen datasets.
      We compare the zero-shot perplexities of MDLM with SEDD and an AR Transformer language model on the validation splits of Penn Tree Bank (PTB), Wikitext, LM1B, Lambada, AG News, and Scientific Papers (Pubmed and Arxiv subsets).
      MDLM consistently outperforms SEDD.
      In some cases, e.g., for Lambada and Scientific Papers, MDLM attains better perplexity than AR.
      We hypothesize that these datasets are farther from OWT, and that diffusion models may be more robust to out-of-domain evaluation due to the unmasking-based objective.
      <br>
      <br>
      <table>
        <caption><em>Zero-shot validation perplexities ( &darr;) of models trained for 524B tokens on OpenWebText.
          All perplexities for diffusion models are upper bounds.</em></caption>
        <tr>
          <th></th>
          <th>PTB</th>
          <th>Wikitext</th>
          <th>LM1B</th>
          <th>Lambada</th>
          <th>AG News</th>
          <th>Pubmed</th>
          <th>Arxiv</th>
        </tr>
        <tr>
          <td>AR (Retrained)</td>
          <td><strong>82.05</strong></td>
          <td><strong>25.75</strong></td>
          <td><strong>51.25</strong></td>
          <td>51.28</td>
          <td><strong>52.09</strong></td>
          <td>49.01</td>
          <td>41.73</td>
        </tr>
        <tr>
          <td>SEDD (Retrained)</td>
          <td>100.09</td>
          <td>34.28</td>
          <td>68.20</td>
          <td>49.86</td>
          <td>62.09</td>
          <td>44.53</td>
          <td>38.48</td>
        </tr>
        <tr>
          <td><strong>MDLM (Ours)</strong></td>
          <td>95.26</td>
          <td>32.83</td>
          <td>67.01</td>
          <td><strong>47.52</strong></td>
          <td>61.15</td>
          <td><strong>41.89</strong></td>
          <td><strong>37.37</strong></td>
        </tr>
      </table>
    </p>
    <h4>Training Considerations for Masked Diffusion</h4>
    <p>One of the key contributions of our work is a well-engineered implementation of masked diffusion
      models. Our experiments demonstrate that these improvements greatly boost performance even for
      methods previously thought to perform poorly, e.g., D3PM . Below we briefly summarize these
      implementation details.
      
      <ol>
        <li>We find that tokenization is critical to performance. Small vocabularies,
      such as the 8k vocabulary in D3PM, result in longer-range dependencies that decrease the
      performance of both diffusion and AR models.
        </li>
        <li>By focusing on masked diffusion, we
      are able to provide a numerically stable implementation of the objective function. Namely, since
      previous formulations of discrete diffusion were constructed to accommodate a wide range of limiting
      distributions, the objective was implemented by materializing the full transition matrices \(Q_t\) and
      posterior probabilities. In contrast, we evaluate
      \(\text{D}_{\text{KL}}[q(\mathbf{z}_s | \mathbf{z}_t,\mathbf{x}) \| p_\theta (\mathbf{z}_s | \mathbf{z}_t)]\) by examining only the
      masked token indices rather than comparing the full true and approximate posterior distributions.
        </li>
        <li>Furthermore, we modernize the architecture for the denoising network relative to D3PM. In lieu
      of the T5 architecture used in D3PM, we use the diffusion transformer (<a href="https://arxiv.org/abs/2212.09748">DiT</a>), which integrates time step conditioning into a standard encoder-only transformer and uses rotary positional embeddings.
        </li>
        <li>In addition, we implement a low-discrepancy sampler
          that reduces the variance of the ELBO, similar to <a href="https://arxiv.org/abs/2107.00630">Kingma et al.</a> and draws correlated samples
          \(t \in [0, 1]\) rather than performing i.i.d. sampling along the batch dimension.</li>
      </ol>
    </p>

    </div>
  </div>
</section>
<!-- End Diffusion Models -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{sahoo2024simple,
      title={Simple and Effective Masked Diffusion Language Models}, 
      author={Subham Sekhar Sahoo and Marianne Arriola and Yair Schiff and Aaron Gokaslan and Edgar Marroquin and Justin T Chiu and Alexander Rush and Volodymyr Kuleshov},
      year={2024},
      eprint={2406.07524},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
</html>
