<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Simple and Effective Masked Diffusion Language Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Simple and Effective <br> Masked Diffusion Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://s-sahoo.github.io" target="_blank">Subham Sekhar Sahoo</a>,</span>
                <span class="author-block">
                  <a href="https://mariannearriola.github.io" target="_blank">Marianne Arriola</a>,</span>
                  <span class="author-block">
                  <a href="https://yair-schiff.github.io" target="_blank">Yair Schiff</a>,</span>
                    <span class="author-block">
                      <a href="https://skylion007.github.io" target="_blank">Aaron Gokaslan</a>,</span>
                      <span class="author-block">
                        <a href="https://emarro.github.io" target="_blank">Edgar Marroquin</a>,</span>
                        <span class="author-block">
                          <a href="https://justinchiu.netlify.app" target="_blank">Justin T Chiu</a>,</span>
                            <span class="author-block">
                              <a href="https://rush-nlp.com" target="_blank">Alexander Rush</a>,</span>
                              <span class="author-block">
                                <a href="https://www.cs.cornell.edu/~kuleshov/" target="_blank">Volodymyr Kuleshov</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Cornell Tech, NY.
                      <!-- <br>Conferance name and year -->
                    </span>
                    
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                        <!-- ArXiv abstract Link -->
                        <span class="link-block">
                          <a href="https://arxiv.org/abs/2406.07524" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="ai ai-arxiv"></i>
                          </span>
                        <span>arXiv</span>

                        <!-- Github link -->
                        <span class="link-block">
                          <a href="https://github.com/kuleshov-group/mdlm" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fab fa-github"></i>
                          </span>
                          <span>Code</span>
                          </a>
                        </span>

                         <!-- Arxiv PDF link -->
                         <!-- <span class="link-block">
                          <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fas fa-file-pdf"></i>
                          </span>
                          <span>Paper</span> -->
                        </a>
                      </span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/mdlm.png" alt="MY ALT TEXT"/>
      <h4 class="subtitle has-text-centered">
        (<em>Left</em>) Our proposed masked diffusion language model (MDLM)
        is trained using a weighted average of masked cross entropy losses.
        (<em>Top Right</em>) In comparison to masked language models (MLM),
        MDLM's objective correspond to a principled variational lower bound,
        and supports generation via ancestral sampling.(<em>Bottom Right</em>)
        Perplexity (PPL) on One Billion Words benchmark.
      </h4>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While diffusion models excel at generating high-quality images, prior work reports
            a significant performance gap between diffusion and autoregressive (AR) methods
            on language modeling. In this work, we show that simple masked discrete diffusion
            is more performant than previously thought. We apply an effective training recipe
            that improves the performance of masked diffusion models and derive a simplified,
            Rao-Blackwellized objective that results in additional improvements. Our objective
            has a simple form—it is a mixture of classical masked language modeling losses—
            and can be used to train encoder-only language models that admit efficient samplers,
            including ones that can generate arbitrary lengths of text semi-autoregressively
            like a traditional language model. On language modeling benchmarks, a range of
            masked diffusion models trained with modern engineering practices achieves a new
            state-of-the-art among diffusion models, and approaches AR perplexity.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Diffusion Models -->
<section class="section" id="DiffusionModels">
  <div class="container is-max-desktop">
  <div class="content is-medium">
    <h2 class="title">Diffusion Models</h2>
    <p>Diffusion models excel at producing realistic, high-quality images and have received significant attention as potential tools for generating discrete data such as text, biological sequences, and graphs. Unlike autoregressive (AR) approaches, diffusion-based methods are not constrained to generate data sequentially, and therefore have the potential to improve long-term planning, controllable generation, and sampling speed. However, discrete diffusion methods exhibit a performance gap relative to AR models, especially in language modeling. The standard measure of language modeling performance is log-likelihood: when controlling for parameter count, prior work reports a sizable log-likelihood gap between AR and diffusion models.
    </p>
  <!-- Continuous Diffusion -->
  
    <h4 class="subtitle">Continuous Diffusion</h3>
    <p>
      Diffusion models are trained to iteratively undo a forward corruption process  q  that corrupts the clean data  $\mathbf{x} \in \mathbb{R}^{n}$  by adding Gaussian noise. In the reverse generation process, the trained model iteratively denoises the Gaussian noise to generate clean inputs that correspond to the input data distribution.
    </p>
    
    <!-- Discrete Diffusion -->
      <h4 class="subtitle">Discrete Diffusion</h3>
      <p>
        Applications of diffusion modeling to discrete data can be categorized into two broad areas. The first involves embedding discrete structures in continuous space and then performing the Gaussian diffusion defined above on these continuous representations [6, 11, 17, 18, 22, 26, 42]. More related to our method are works that define a diffusion process directly on discrete structures. D3PM [1] introduces a framework with a Markov forward process \( q(z_t|z_{t−1}) = \text{Cat}(z_t; Q_t z_{t−1}) \), defined by the multiplication of matrices \( Q_t \in \mathbb{R}^{n \times n} \) over \( T \) discrete time steps. The matrix \( Q_t \) is designed such that \( Q_T \cdot Q_{T-1} \cdots Q_1 \mathbf{x} \) converges to a stationary distribution.
      </p>
  </div>
  </div>

</section>
<!-- End Diffusion Models -->

<!-- Simple Masked Diffusion Models -->
<section class="section" id="SimpleMaskedDiffusionModels">
  <div class="container is-max-desktop">
  <div class="content is-medium">
    <h2 class="title">Simple Masked Diffusion Models</h2>
    <p>While previous work on discrete diffusion supports general forward processes (e.g., general \(Q_t\) in
    D3PM), absorbing state (i.e., masking) diffusion consistently achieves the best performance [1, 25 ].
    In this work, instead of supporting general noise processes, we focus on masking and derive tight
    Rao-Blackwellized objectives that outperform general approaches and do not require CTMC theory.
    We denote our
    overall approach as Masked Diffusion Language Models (MDLM).
    </p>
    <h4 class="subtitle">Interpolating Masked Diffusion</h4>
    <p>We restrict our attention to forward processes q that interpolate between clean data \(\mathbf{x} \in \mathcal{V}\) and a target
      distribution \(\text{Cat}(.; \boldsymbol{\mathit{\pi}})\), forming a direct extension of Gaussian diffusion where the intermediate sample is an interpolation between the clean data and the white noise. The \(q\) define a sequence
      of increasingly noisy latent variables \(\mathbf{z}_t \in \mathcal{V}\), where the time step \(t\) runs from \(t = 0\) (least noisy) to \(t = 1\)
      (most noisy). The marginal of \(\mathbf{z}_t\) conditioned on \(\mathbf{x}\) at time \(t\) is
      \(q(\mathbf{z}_t|x) = Cat(\mathbf{z}_t;\alpha_t \mathbf{x}+(1 - \alpha_t) \boldsymbol{\mathit{\pi}})\),
      where \(\alpha_t \in [0, 1]\) is a strictly decreasing function in t, with \(\alpha_{t=0} \approx 0\) and \(\alpha_{t=1} \approx 1\).
      In masked (i.e., absorbing state) diffusion, we set \(\boldsymbol{\mathit{\pi}} = \mathbf{m}\). At each noising step, \(t\), the input \(\mathbf{x}\) transitions to a `masked' state
      \(\mathbf{m}\) with some probability. [TODO: define M]
    </p>
    
    <h4>Reverse process</h4>
    <p>The specific parameterization for \(p_\theta(\mathbf{z}_s | \mathbf{z}_t)\) that we use is
      \begin{align}\label{eqn:approx_posterior}
          p_\theta(\mathbf{z}_s | \mathbf{z}_t) =
          q(\mathbf{z}_s | \mathbf{z}_t, \mathbf{x} = \mathbf {x}_\theta (\mathbf{z}_t, t)) =
          \begin{cases}
            \text{Cat} (\mathbf{z}_s; \mathbf{z}_t), & \mathbf{z}_t \neq \mathbf{m}, \\
            \text{Cat} \left( \mathbf{z}_s; \frac{
              (1 - \alpha_s)\mathbf{m} + (\alpha_s - \alpha_t) \mathbf{x}_\theta (\mathbf{z}_t, t)}{1 - \alpha_t}\right). & \mathbf{z}_t= \mathbf{m},
          \end{cases}
      \end{align}
      Furthermore, we induce 2 key properties of the absorbing state diffusion process into our denoising
      model, \( \mathbf{x}_\theta (\mathbf{z}_t, t) \): an input token remains unchanged during reverse diffusion, and the clean input
      is never masked We implement these as substitutions to the output of \( \mathbf{x}_\theta (\mathbf{z}_t, t) \), hence we call our
      parameterization <b>SUBS</b>.
      <ol>
        <li>
          <b>Zero Masking Probabilities:</b> First, notice that by definition, \( \langle \mathbf{x}, \mathbf{m} \rangle = 0 \).
          For this reason, we design the denoising network such that 
          \( \langle \mathbf{x}_\theta (\mathbf{z}_t,t),\mathbf{m} \rangle = 0 \), i.e., we substitute the logit index corresponding to the [MASK] token with \(\infty\).
        </li>
        <li>
          <b>Carry-Over Unmasking:</b> Second, if \( \mathbf{z}_t \) is unmasked, then we desire \( \mathbf{x}_\theta (\mathbf{z}_t, t) = \mathbf{z}_t \), i.e., unmasked latents are "carried over". We accomplish this by substituting the output of our network to simply copy unmasked inputs.
        </li>
      </ol>
    </p>
    <h4>Loss</h4>
    <p>
      <!-- Note that the Negative Evidence Lower Bound (NELBO) takes the following form for a diffusion model: \( \mathcal{L}_{\text{reconstruction}} + \mathcal{L}_{\text{diffusion}} + \mathcal{L}_{\text{prior}}\). As described in the paper, SUBS parameterization reduces the  \( \mathcal{L}_{\text{reconstruction}} \) to \(0\) and \(\alpha_{t = 1} = 0\) ensures that  \( \mathcal{L}_{\text{prior}} = 0 \). -->
      For a sequence \(\mathbf{x}^{1: L}\) of length \(L\), SUBS parameterization simplifies the Negative Evidence Lower Bound (NELBO) to the following:
      \begin{align}
          \mathcal{L}_{\text{NELBO}}^\infty = \mathbb{E}_{q}\int_{t=0}^{t=1} \frac{\alpha_{t}'}{1 - \alpha_t} \sum_{\ell = 1}^{L} \log \langle \mathbf {x}_\theta^\ell(\mathbf{z}_t), \mathbf{x}^\ell \rangle \text{d} t
      \end{align}
      where \(\alpha_{t}'\) denotes the time derivative of \(\alpha_{t}\). As shown in the table below, MDLM outperforms the previous diffusion models and almost matches
      the performance of the AR models.

      <!-- <table>
        <caption>Test perplexities (PPL; &darr;) on LM1B. Best diffusion value is bolded. All perplexities for diffusion models are upper bounds.</caption>
        <thead>
          <tr>
            <th></th>
            <th>PPL (&darr;)</th>
            <th>OWT (&darr;)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>SEDD</td>
            <td>32.79</td>
            <td>24.10</td>
          </tr>
          <tr>
            <td>AR (Retrained)</td>
            <td>20.86</td>
            <td>17.54</td>
          </tr>
          <tr>
            <td><strong>MDLM (Ours)</strong></td>
            <td> <b>23.00</b></td>
            <td><b>23.21</b></td>
          </tr>
        </tbody>
      </table> -->
      <table>
        <caption>Test perplexities (PPL; &darr;) on LM1B. &dagger;. Best diffusion value is bolded.</caption>
        <thead>
          <tr>
            <th></th>
            <th></th>
            <th>Parameters</th>
            <th>PPL (&darr;)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="2"><i>Autoregressive</i></td>
            <td>Transformer-X Base</td>
            <td>0.46B</td>
            <td>23.5</td>
          </tr>
          <tr>
            <td><i>OmniNet<sub>T</sub></i></td>
            <td>100M</td>
            <td>21.5</td>
          </tr>
          <tr>
            <td rowspan="5"><i>Diffusion</i></td>
            <td>BERT-Mouth</td>
            <td>110M</td>
            <td>&le;142.89</td>
          </tr>
          <tr>
            <td>D3PM (absorb)</td>
            <td>70M</td>
            <td>&le;77.50</td>
          </tr>
          <tr>
            <td>Diffusion-LM</td>
            <td>80M</td>
            <td>&le;118.62</td>
          </tr>
          <tr>
            <td>DiffusionBert</td>
            <td>110M</td>
            <td>&le;63.78</td>
          </tr>
          <tr>
            <td>SEDD</td>
            <td>110M</td>
            <td>&le;32.79</td>
          </tr>
          <tr style="border-top: 0.1em solid;">
            <td rowspan="2"><i>Autoregressive (Retrained)</i></td>
            <td>Transformer (33B tokens)</td>
            <td rowspan="2">110M</td>
            <td>22.32</td>
          </tr>
          <tr>
            <td>Transformer (327B tokens)</td>
            <td>20.86</td>
          </tr>
          <tr>
            <td rowspan="2"><i>Diffusion (Ours)</i></td>
            <td><strong>MDLM </strong> (33B tokens)</td>
            <td rowspan="2">110M</td>
            <td>&le;27.04</td>
          </tr>
          <tr>
            <td><strong>MDLM </strong> (327B tokens)</td>
            <td>&le;<b>23.00</b></td>
          </tr>
        </tbody>
      </table>
      We also explore models' ability to generalize by taking models trained on OWT and evaluating how well they model unseen datasets.
      We compare the zero-shot perplexities of MDLM with SEDD and an AR Transformer language model on the validation splits of Penn Tree Bank (PTB; [28 ]), Wikitext [29], LM1B, Lambada [31 ], AG News [ 51 ], and Scientific Papers (Pubmed and Arxiv subsets; [7]).
      MDLM consistently outperforms the SEDD.
      In some cases, e.g., for Lambada and Scientific Papers, MDLM attains better perplexity than AR.
      We hypothesize that these datasets are farther from OWT, and that diffusion models may be more robust to out-of-domain evaluation due to the unmasking-based objective.
      <table>
        <caption>Zero-shot validation perplexities ( &darr;) of models trained for 524B tokens on OpenWebText.
          All perplexities for diffusion models are upper bounds.</caption>
        <tr>
          <th></th>
          <th>PTB</th>
          <th>Wikitext</th>
          <th>LM1B</th>
          <th>Lambada</th>
          <th>AG News</th>
          <th>Pubmed</th>
          <th>Arxiv</th>
        </tr>
        <tr>
          <td>AR (Retrained)</td>
          <td><strong>82.05</strong></td>
          <td><strong>25.75</strong></td>
          <td><strong>51.25</strong></td>
          <td>51.28</td>
          <td><strong>52.09</strong></td>
          <td>49.01</td>
          <td>41.73</td>
        </tr>
        <tr>
          <td>SEDD (Retrained)</td>
          <td>100.09</td>
          <td>34.28</td>
          <td>68.20</td>
          <td>49.86</td>
          <td>62.09</td>
          <td>44.53</td>
          <td>38.48</td>
        </tr>
        <tr>
          <td><strong>MDLM (Ours)</strong></td>
          <td>95.26</td>
          <td>32.83</td>
          <td>67.01</td>
          <td><strong>47.52</strong></td>
          <td>61.15</td>
          <td><strong>41.89</strong></td>
          <td><strong>37.37</strong></td>
        </tr>
      </table>

    </p>
    </div>
  </div>
</div>

</section>
<!-- End Diffusion Models -->

<section class="section" id="Samplers">
  <div class="container is-max-desktop">
  <div class="content is-medium">
    <h2 class="title">Samplers</h2>
    <h4>Efficient Ancestral Sampling</h4>
    <p>Note that in the reverse process, unmasked tokens remain unchanged. Thus, if no new tokens in 
      the intermediate latents \(\mathbf{z}^{1:L}\)
      become unmasked (which can occur often in early denoising stages for large \(T\)), then z1:L
      s = z1:L
      t .
      Additionally if the denoising model, xθ (z1:L
      t ) is not conditioned on time, then we can simply draw
      a new sample from pθ (z1:L
      s−1/T |z1:L
      s ) using the previously computed and cached value xθ (z1:L
      t ). This
      means we have effectively ‘skipped’ over the time step s, saving a function call to the denoising
      network. Note that SEDD [ 25 ] does not support this caching because the denoising network models
      time-dependent rates, which requires conditioning on time.
      
      <img src="static/images/caching.png" alt="caching" style="width:50%;height:50%;">
    </p>
    <h4>Semi-Autoregressive Masked Diffusion Language Models</h4>
    <p>
      Our method also admits an effective semi-autoregressive (SAR) decoding method that allows the model
      to generate sequences of arbitrary length. Let  ̃x1:L represent the output from sampling a sequence of
      L tokens using the reverse diffusion process described above. To generate additional L′ < L tokens, we
      propose a generation algorithm in which the latter L−L′ tokens  ̃xL′ :L−L′
      are used as a prefix for an
      additional round of generation. Given the carry-over unmasking described in Sec. 3.2.3, these prefix to-
      kens will simply be copied over at each decoding step. The remaining tokens are generated as above with
      zℓ
      s ∼ pθ (zℓ
      s | zL′ :L+L′
      t ) for all ℓ ∈ {L+1,...,L+L′}, with zL′ :L−L′
      1 initialized to  ̃xL′ :L−L′
      as opposed
      to being initialized as masked tokens m. At the end of this process, we have produced L+L′ tokens
      concat[ ̃x1:L, ̃xL+1:L+L′
      ], where concat[·] denotes concatenation along the sequence length dimension.
      This process can repeat indefinitely, with the prefix shifted for every new round of generation.

      <table>
        <thead>
          <tr>
            <th></th>
            <th>Gen. PPL (&darr;)</th>
            <th>Sec/Seq (&darr;)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>SSD-LM</td>
            <td>35.43</td>
            <td>2473.9</td>
          </tr>
          <tr>
            <td>\algo{} (Ours)</td>
            <td><b>27.18</b></td>
            <td><b>89.3</b></td>
          </tr>
        </tbody>
      </table>
    </p>
<section class="section" id="Samplers">
  <div class="container is-max-desktop">
  <div class="content is-medium">
    <h2 class="title">Training Considerations for Masked Diffusion    </h2>
    <p>      One of the key contributions of our work is a well-engineered implementation of masked diffusion
      models. Our experiments demonstrate that these improvements greatly boost performance even for
      methods previously thought to perform poorly, e.g., Austin et al. [1] . Below we briefly summarize these
      implementation details. First, we find that tokenization is critical to performance. Small vocabularies,
      such as the 8k vocabulary in Austin et al. [1], result in longer-range dependencies that decrease the
      performance of both diffusion and AR models. Additionally, by focusing on masked diffusion, we
      are able to provide a numerically stable implementation of the objective function. Namely, since
      previous formulations of discrete diffusion were constructed to accommodate a wide range of limiting
      distributions [1], the objective was implemented by materializing the full transition matrices  ̄Qt and
      posterior probabilities. In contrast, we evaluate DKL[q(zs | \mathbf{z}_t,x)||pθ (zs | \mathbf{z}_t)] by examining only the
      masked token indices rather than comparing the full true and approximate posterior distributions.
      Furthermore, we modernize the architecture for the denoising network relative to D3PM [ 1 ]. In lieu
      of the T5 architecture used in D3PM, we use the diffusion transformer (DiT) introduced in Peebles
      & Xie [32] , which integrates time step conditioning into a standard encoder-only transformer [47 ]
      and uses rotary positional embeddings [ 43 ]. In addition, we implement a low-discrepancy sampler
      that reduces the variance of the ELBO, similar to Kingma et al. [21] and draws correlated samples rather than performing i.i.d. sampling.
    </p>
  </div>
  </div>
</section>

<!-- End paper conclusion -->
<section class="section" id="Conclusion">
  <div class="container is-max-desktop">
    <h2 class="title">Conclusion</h2>
    <div class="content is-medium">
      <p>
      TODO 
      </p>
    </div>
  </div>
<!-- End paper conclusion -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{sahoo2024simple,
      title={Simple and Effective Masked Diffusion Language Models}, 
      author={Subham Sekhar Sahoo and Marianne Arriola and Yair Schiff and Aaron Gokaslan and Edgar Marroquin and Justin T Chiu and Alexander Rush and Volodymyr Kuleshov},
      year={2024},
      eprint={2406.07524},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
